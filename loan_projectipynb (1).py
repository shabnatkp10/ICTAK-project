# -*- coding: utf-8 -*-
"""loan_projectipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lMGbBReK4HLek43u97E-42AUke0aD9Rk

import libraries
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

"""load the training and test data set"""

train = pd.read_csv('/content/train_ctrUa4K.csv')
train

test = pd.read_csv('/content/test_lAUu6dG.csv')
test

"""Exploratory Data Analysis"""

train.info()

train.isnull().sum()

train.describe()

test.shape

train.shape

test.duplicated().sum()

train.dtypes

train.duplicated().sum()

#Check null values / Empty Cells in each column
test.isna().sum()

train.isna().sum()

"""handling missing values"""

# Define columns by type for easy handling
categorical_cols = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']
numerical_cols = ['LoanAmount', 'Loan_Amount_Term']

# Fill missing values in train dataset
for col in categorical_cols:
    train[col].fillna(train[col].mode()[0], inplace=True)

for col in numerical_cols:
    train[col].fillna(train[col].median(), inplace=True)

# Fill missing values in test dataset
for col in categorical_cols:
    test[col].fillna(test[col].mode()[0], inplace=True)

for col in numerical_cols:
    test[col].fillna(test[col].median(), inplace=True)

# Perform one-hot encoding for categorical variables, dropping the first level to avoid multicollinearity
train = pd.get_dummies(train, columns=['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'], drop_first=True)
test = pd.get_dummies(test, columns=['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'], drop_first=True)

# Create a new feature for total income in both train and test datasets
train['TotalIncome'] = train['ApplicantIncome'] + train['CoapplicantIncome']
test['TotalIncome'] = test['ApplicantIncome'] + test['CoapplicantIncome']

# Drop original income columns if not needed
train.drop(['ApplicantIncome', 'CoapplicantIncome'], axis=1, inplace=True)
test.drop(['ApplicantIncome', 'CoapplicantIncome'], axis=1, inplace=True)

"""scaling numerical features"""

from sklearn.preprocessing import StandardScaler

# Assuming 'numerical_cols' is defined as in the previous code
numerical_cols = ['LoanAmount', 'Loan_Amount_Term', 'TotalIncome']

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the numerical features in the training set
train[numerical_cols] = scaler.fit_transform(train[numerical_cols])

# Transform the numerical features in the test set using the same scaler
test[numerical_cols] = scaler.transform(test[numerical_cols])

# Verify no missing values remain
print(train.isnull().sum().sum())  # Should print 0
print(test.isnull().sum().sum())   # Should print 0

# Check for column consistency between train and test datasets
print(train.columns)
print(test.columns)

"""modelling

Modeling
"""

from sklearn.model_selection import train_test_split

# Separate features and target variable
X = train.drop(columns=['Loan_ID', 'Loan_Status'])
y = train['Loan_Status'].apply(lambda x: 1 if x == 'Y' else 0)  # Convert target to 1 (Yes) and 0 (No)

# Split into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)
    print(f"{name} Accuracy: {accuracy:.4f}")

from sklearn.ensemble import VotingClassifier

# Ensemble using VotingClassifier
voting_clf = VotingClassifier(
    estimators=[('lr', LogisticRegression(max_iter=1000)),
                ('rf', RandomForestClassifier(random_state=42)),
                ('gb', GradientBoostingClassifier(random_state=42))],
    voting='hard'
)

voting_clf.fit(X_train, y_train)
voting_accuracy = voting_clf.score(X_val, y_val)
print("Voting Classifier Accuracy:", voting_accuracy)

from sklearn.model_selection import GridSearchCV

# Example for Random Forest hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best accuracy:", grid_search.best_score_)

"""best model selecting"""

best_model = RandomForestClassifier(random_state=42)
best_model.fit(X, y)

"""Predict on the Test Data"""

# Prepare test data by dropping Loan_ID and aligning with the training data
X_test = test.drop(columns=['Loan_ID'])

# Get the feature names from the training data
training_features = best_model.feature_names_in_

# Ensure the test data has the same columns as the training data
X_test = X_test[training_features]

test_pred = best_model.predict(X_test)
test_pred = ['Y' if pred == 1 else 'N' for pred in test_pred]

# Create the submission DataFrame
submission = pd.DataFrame({
    'Loan_ID': test['Loan_ID'],
    'Loan_Status': test_pred
})

# Save the submission file
submission.to_csv('loan_eligibility_submission.csv', index=False)
print("Submission file created successfully!")

# Use a Support Vector Classifier (SVC)
from sklearn.svm import SVC

# Initialize the SVC model
svc_model = SVC(random_state=42)

# Train the SVC model
svc_model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_svc = svc_model.predict(X_val)

# Evaluate the SVC model
svc_accuracy = accuracy_score(y_val, y_pred_svc)
print(f"SVC Accuracy: {svc_accuracy:.4f}")

# Predict on the Test Data using SVC
test_pred_svc = svc_model.predict(X_test)
test_pred_svc = ['Y' if pred == 1 else 'N' for pred in test_pred_svc]

# Create the submission DataFrame for SVC
submission_svc = pd.DataFrame({
    'Loan_ID': test['Loan_ID'],
    'Loan_Status': test_pred_svc
})

# Save the submission file for SVC
submission_svc.to_csv('loan_eligibility_submission_svc.csv', index=False)
print("SVC Submission file created successfully!")



# prompt: use gradient boosting machine model

# Initialize and train the Gradient Boosting Classifier
gb_model = GradientBoostingClassifier(random_state=42)
gb_model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_gb = gb_model.predict(X_val)

# Evaluate the Gradient Boosting model
gb_accuracy = accuracy_score(y_val, y_pred_gb)
print(f"Gradient Boosting Accuracy: {gb_accuracy:.4f}")

# Predict on the Test Data using Gradient Boosting
test_pred_gb = gb_model.predict(X_test)
test_pred_gb = ['Y' if pred == 1 else 'N' for pred in test_pred_gb]

# Create the submission DataFrame for Gradient Boosting
submission_gb = pd.DataFrame({
    'Loan_ID': test['Loan_ID'],
    'Loan_Status': test_pred_gb
})

# Save the submission file for Gradient Boosting
submission_gb.to_csv('loan_eligibility_submission_gb.csv', index=False)
print("Gradient Boosting Submission file created successfully!")

# Example for Random Forest hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best accuracy:", grid_search.best_score_)

# You can further explore other models and their hyperparameters
# Example: hyperparameter tuning for GradientBoostingClassifier
param_grid_gb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 0.9, 1.0]
}

grid_search_gb = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid_gb, cv=5, scoring='accuracy')
grid_search_gb.fit(X_train, y_train)

print("Best parameters for Gradient Boosting:", grid_search_gb.best_params_)
print("Best accuracy for Gradient Boosting:", grid_search_gb.best_score_)